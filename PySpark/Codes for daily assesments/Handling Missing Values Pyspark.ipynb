{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8278a18-9427-4ace-962e-3ea2363830a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|   Krish|  31|        10| 30000|\n|Sudhashu|  30|         8| 25000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|  Mahesh|NULL|      NULL| 40000|\n|    NULL|  34|        10| 38000|\n|    NULL|  36|      NULL|  NULL|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1=spark.read.csv(\"/FileStore/tables/Teachers.csv\",header=True,inferSchema=True)\n",
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c133c9d-9132-4dd1-a29b-52592fb927bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n|    Name|age|Experience|Salary|\n+--------+---+----------+------+\n|   Krish| 31|        10| 30000|\n|Sudhashu| 30|         8| 25000|\n|   Sunny| 29|         4| 20000|\n|    Paul| 24|         3| 20000|\n|  Harsha| 21|         1| 15000|\n| Shubham| 23|         2| 18000|\n+--------+---+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows based on null values\n",
    "df_pyspark1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a1c275-b464-4328-8c17-854ab60338ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop() has the following parameters â€”\n",
    "how, thresh, and subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7767abf-c79e-43bd-9c0b-0c32711005d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|   Krish|  31|        10| 30000|\n|Sudhashu|  30|         8| 25000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|  Mahesh|NULL|      NULL| 40000|\n|    NULL|  34|        10| 38000|\n|    NULL|  36|      NULL|  NULL|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# if all values in rows are null then drop # default any - How\n",
    "df_pyspark1.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5c2b12-f1a7-48e1-9d79-c07bad05ec48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|   Krish|  31|        10| 30000|\n|Sudhashu|  30|         8| 25000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|  Mahesh|NULL|      NULL| 40000|\n|    NULL|  34|        10| 38000|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#atleast 2 non null values should be present. -Thresh\n",
    "df_pyspark1.na.drop(how=\"any\",thresh=2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0d80e5-5d6b-422f-82a3-e10065330cd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|   Krish|  31|        10| 30000|\n|Sudhashu|  30|         8| 25000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|  Mahesh|NULL|      NULL| 40000|\n|    NULL|  34|        10| 38000|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# only in that column rows get deleted - Subset\n",
    "df_pyspark1.na.drop(how=\"any\",subset=[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d9472d-825f-4923-85a8-0babccc710c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|    NULL|  36|      NULL|  NULL|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n|Sudhashu|  30|         8| 25000|\n|   Krish|  31|        10| 30000|\n|    NULL|  34|        10| 38000|\n|  Mahesh|NULL|      NULL| 40000|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort based on single column\n",
    "df_pyspark1.sort(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db64fad-85a8-4a07-b72b-9ad7c5af8176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|  Mahesh|NULL|      NULL| 40000|\n|    NULL|  34|        10| 38000|\n|   Krish|  31|        10| 30000|\n|Sudhashu|  30|         8| 25000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n| Shubham|  23|         2| 18000|\n|  Harsha|  21|         1| 15000|\n|    NULL|  36|      NULL|  NULL|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# sort based on descending order\n",
    "df_pyspark1.sort(df_pyspark1[\"salary\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1094d5-2b8b-4a20-bd51-7f4e147a70af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|    NULL|  36|      NULL|  NULL|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|    Paul|  24|         3| 20000|\n|   Sunny|  29|         4| 20000|\n|Sudhashu|  30|         8| 25000|\n|   Krish|  31|        10| 30000|\n|    NULL|  34|        10| 38000|\n|  Mahesh|NULL|      NULL| 40000|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort based on first column then second column\n",
    "df_pyspark1.sort(\"salary\",\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7db668-b5a9-4a74-9e14-e69c00d40fda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n|    Name| age|Experience|Salary|\n+--------+----+----------+------+\n|    NULL|  36|      NULL|  NULL|\n|  Harsha|  21|         1| 15000|\n| Shubham|  23|         2| 18000|\n|   Sunny|  29|         4| 20000|\n|    Paul|  24|         3| 20000|\n|Sudhashu|  30|         8| 25000|\n|   Krish|  31|        10| 30000|\n|    NULL|  34|        10| 38000|\n|  Mahesh|NULL|      NULL| 40000|\n+--------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#orderBy\n",
    "df_pyspark1.orderBy(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655053da-3cb2-4b13-ab02-a3c2648ad245",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling Missing Values Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
